transformers == 4.32.0
accelerate >= 0.20.1
datasets
evaluate
scikit-learn
deepspeed
packaging
git+https://github.com/Dao-AILab/flash-attention#subdirectory=csrc/layer_norm

flash-attn >= 2.2.3